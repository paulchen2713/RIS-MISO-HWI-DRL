{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "print(env.reset())  # calling env.reset() will give us a random state, e.g. [-0.59585701  0.        ]\n",
    "\n",
    "# Q-Learning settings\n",
    "LEARNING_RATE = 0.1  # [0.1, 0.0001]\n",
    "DISCOUNT = 0.95      # [0.95, 0.99]\n",
    "EPISODES = 25_000     # 25_000\n",
    "SHOW_EVERY = 1000    # 1000\n",
    "\n",
    "# Quantization settings\n",
    "# DISCRETE_OS_SIZE = [20, 20]  # use 20 groups/buckets for each range. (20 units) \n",
    "DISCRETE_OS_SIZE = [40] * len(env.observation_space.high)\n",
    "# [(0.6 - (-1.2)) / 20, (0.07 - (-0.07)) / 20] == [1.8 / 20, 0.14 / 20] == [0.09, 0.007]\n",
    "discrete_os_window_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
    "\n",
    "# Epsilon-Greedy Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES // 2 \n",
    "epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING + 1)\n",
    "\n",
    "# For stats \n",
    "epi_rewards = [] \n",
    "aggr_epi_rewards = {'epi': [], 'avg': [], 'max': [], 'min': []}\n",
    "STATS_EVERY = 100\n",
    "SAVE_QTABLE_EVERY = 100\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_name(var):\n",
    "    import inspect\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "\n",
    "def log(*argv):\n",
    "    import torch\n",
    "    for arg in argv:\n",
    "        print(f\"-\"*75)\n",
    "        print(f\"{retrieve_name(arg)}\")\n",
    "        print(f\"content: \")\n",
    "        print(arg)\n",
    "        print(f\"type: {type(arg)}\")\n",
    "        if isinstance(arg, np.ndarray) or isinstance(arg, torch.Tensor): \n",
    "            print(f\"shape: {arg.shape}\")\n",
    "        elif isinstance(arg, list) or isinstance(arg, str) or isinstance(arg, dict):\n",
    "            print(f\"len: {len(arg)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of actions: {env.action_space.n}\")  # 3 \n",
    "# there are \"3\" actions we can pass: 0 means push left, 1 is stay still, and 2 means push right\n",
    "print(f\"random action: {env.action_space.sample()}\")  # 0, 1, or 2\n",
    "\n",
    "# we can query the enviornment to find out the possible ranges for each of these state values\n",
    "print(f\"state values range: \") \n",
    "print(f\"  {env.observation_space.high}\")  # [0.6  0.07]\n",
    "print(f\"  {env.observation_space.low}\")   # [-1.2  -0.07]\n",
    "\n",
    "print(f\"len(env.observation_space.high): {len(env.observation_space.high)}\")  # \n",
    "print(f\"discrete observation space window size: {discrete_os_window_size}\")   # [0.09  0.007]\n",
    "\n",
    "size = DISCRETE_OS_SIZE + [env.action_space.n]\n",
    "print(f\"q_table size: {size}\")  # [20, 20, 3] is a 20x20x3 shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d34f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=DISCRETE_OS_SIZE + [env.action_space.n])\n",
    "# log(q_table)\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = (list(state) - env.observation_space.low) / discrete_os_window_size\n",
    "    # log(discrete_state, state, env.observation_space.low, discrete_os_window_size)\n",
    "    # we use this tuple to look up the 3 Q-values for the available actions in the 'q_table'\n",
    "    return tuple(discrete_state.astype(np.int8)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_state = get_discrete_state(env.observation_space.sample())\n",
    "print(type(test_state))                       # <class 'tuple'>\n",
    "print(test_state)                             # (6, 10)\n",
    "test_action = np.argmax(q_table[test_state])  \n",
    "print(type(test_state + (test_action, )))     # <class 'tuple'>\n",
    "print(test_state + (test_action, ))           # (6, 10, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    episode_reward = 0  # current episode reward\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        render = True\n",
    "        print(episode)\n",
    "    else:\n",
    "        render = False\n",
    "    \n",
    "    # Get the initial state values from env.reset() and store it to 'discrete_state'\n",
    "    discrete_state = get_discrete_state(env.observation_space.sample())\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Take an action\n",
    "        if epsilon > 0 and np.random.random() > epsilon:\n",
    "            # Get action from Q-table\n",
    "            action = np.argmax(q_table[discrete_state])  # get the index of the greatest Q-value in the q_table\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action=action)\n",
    "        # e.g. reward: -1.0, state := [position, velocity] == [-0.5519343  -0.01300341]\n",
    "        episode_reward += reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % SHOW_EVERY == 0:\n",
    "            # print(reward, new_state)  \n",
    "            env.render()\n",
    "\n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        if not done:\n",
    "\n",
    "            # Maximum possible Q-value in next step (for new state)\n",
    "            max_future_q = np.max(q_table[new_discrete_state])  # get the max value, not the index\n",
    "\n",
    "            # Current Q value (for current state and performed action)\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            # And here's our equation for a new Q value for current state and action:\n",
    "            #   Q_{new}(s_t, a_t) <-- (1 - \\alpha) \\cdot Q(s_t, a_t) +    \\alpha      \\cdot (  r_t   +    \\gamma      \\cdot \\max_{a} Q(s_{t + 1}, a))\n",
    "            #       new-Q-value                            old-value    learning-rate         reward   discount-fator       estimate-of-future-value\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            # Update Q table with new Q value\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "\n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q-value with reward directly\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            # q_table[discrete_state + (action, )] = reward\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "        \n",
    "        # Updating the old state with the new state\n",
    "        discrete_state = new_discrete_state\n",
    "    \n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if START_EPSILON_DECAYING <= episode <= END_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "\n",
    "    # Save stats for further analysis\n",
    "    epi_rewards.append(episode_reward)\n",
    "    if not episode % STATS_EVERY:\n",
    "        average_reward = sum(epi_rewards[-STATS_EVERY:]) / STATS_EVERY  # running average of past 'STATS_EVERY' number of rewards \n",
    "        aggr_epi_rewards['epi'].append(episode)\n",
    "        aggr_epi_rewards['avg'].append(average_reward)\n",
    "        aggr_epi_rewards['max'].append(max(epi_rewards[-STATS_EVERY:]))\n",
    "        aggr_epi_rewards['min'].append(min(epi_rewards[-STATS_EVERY:]))\n",
    "        # ':>5d' pad decimal with zeros (left padding, width 5), ':>4.1f' format float 1 decimal places (left padding, width 4)\n",
    "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
    "\n",
    "    # Save the q_table for every SAVE_QTABLE_EVERY number of episodes\n",
    "    # if episode % SAVE_QTABLE_EVERY == 0:\n",
    "    #     np.save(f\"qtables/{episode}-qtable.npy\", q_table) \n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac781613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reward figure \n",
    "plt.plot(aggr_epi_rewards['epi'], aggr_epi_rewards['avg'], label=\"avg rewards\")\n",
    "plt.plot(aggr_epi_rewards['epi'], aggr_epi_rewards['max'], label=\"max rewards\")\n",
    "plt.plot(aggr_epi_rewards['epi'], aggr_epi_rewards['min'], label=\"min rewards\")\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(f\"episodes\")\n",
    "plt.ylabel(f\"rewards\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig(f\"figures/avg-max-min-rewards.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
